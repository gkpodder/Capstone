\documentclass[12pt, titlepage]{article}

\usepackage{booktabs}
\usepackage{tabularx}
\usepackage{hyperref}
\hypersetup{
    colorlinks,
    citecolor=blue,
    filecolor=black,
    linkcolor=red,
    urlcolor=blue
}
\usepackage[round]{natbib}

\input{../Comments}
\input{../Common}

\begin{document}

\title{System Verification and Validation Plan for \progname{}} 
\author{\authname}
\date{\today}
	
\maketitle

\pagenumbering{roman}

\section*{Revision History}

\begin{tabularx}{\textwidth}{p{3cm}p{2cm}X}
\toprule {\bf Date} & {\bf Version} & {\bf Notes}\\
\midrule
Date 1 & 1.0 & Notes\\
Date 2 & 1.1 & Notes\\
\bottomrule
\end{tabularx}

~\\
\wss{The intention of the VnV plan is to increase confidence in the software.
However, this does not mean listing every verification and validation technique
that has ever been devised.  The VnV plan should also be a \textbf{feasible}
plan. Execution of the plan should be possible with the time and team available.
If the full plan cannot be completed during the time available, it can either be
modified to ``fake it'', or a better solution is to add a section describing
what work has been completed and what work is still planned for the future.}

\wss{The VnV plan is typically started after the requirements stage, but before
the design stage.  This means that the sections related to unit testing cannot
initially be completed.  The sections will be filled in after the design stage
is complete.  the final version of the VnV plan should have all sections filled
in.}

\newpage

\tableofcontents

\listoftables
\wss{Remove this section if it isn't needed}

\listoffigures
\wss{Remove this section if it isn't needed}

\newpage

\section{Symbols, Abbreviations, and Acronyms}

\renewcommand{\arraystretch}{1.2}
\begin{tabular}{l l} 
  \toprule		
  \textbf{symbol} & \textbf{description}\\
  \midrule 
  T & Test\\
  \bottomrule
\end{tabular}\\

% \wss{symbols, abbreviations, or acronyms --- you can simply reference the SRS
%   \citep{SRS} tables, if appropriate}

\wss{Remove this section if it isn't needed}

\newpage

\pagenumbering{arabic}

This document ... \wss{provide an introductory blurb and roadmap of the
  Verification and Validation plan}

\section{General Information}

\subsection{Summary}
The aim of this document is to layout the verification and validation plan for
Project Proxi. Project Proxi is an AI powered voice assistant which allows it's 
users to use their voice to interface with their computer. The general target 
demographic for Project Proxi is seniors and individuals with disabilities who 
wouldn't otherwise be able to use their computers for varying tasks. 

\subsection{Objectives}
The main objective of the Verification and Validation (V\&V) plan is to confirm
that Project Proxi satisfies it's stated functional and non functional 
requirements; and fulfuills its primary purpose of enhancing computer 
accessibility for seniors and individuals with disabilities.

The principal objectives that this VnV plan aims to meet are:
\begin{itemize}
  \item \textbf{Verify Functional Correctness:} Ensure that the features 
  described in the SRS are implemented and behave as expected through Unit, 
  Integration and System Level testing.
  \item \textbf{Validate Usability and Accessibility:} demonstrate that users 
  within the target demographic can effectively operate the system using natural
  language commands while meeting the relevant accessibility standards like 
  WCAG 2.
  \item \textbf{Assess Performance and reliability:} Confirm the application 
  meets performance, stability and error goals as defined in the SRS document.
  \item \textbf{Hardware Compatibility Testing:} Meet verficiation goals on a 
  variety of consumer hardware devices. Speacialized or adaptive devices will
  not be included; only commercially available Windows/MacOS Computers.
\end{itemize}

Out of scope objectives include Extensive long-term field usability studies, 
comprehensive privacy compliance, and Speacialized Hardware Testing. The project
will assume that any external libraries have already been verfied by their 
respective development team. These exclusions are justified given the project's 
limited time, budget and academic context. Verification efforts are therefore 
directed towards objectives that help meet core software requirements.


\subsection{Challenge Level and Extras}

This project does not include a designated challenge level, as its primary focus
 is on the development and validation of a functional, accessible, and 
 user-friendly AI-powered desktop assistant. The project scope aligns with the 
 general expectations of the capstone course and emphasizes reliability, 
 usability, and compliance with accessibility standards rather than advanced 
 AI research or complex algorithmic innovation.


This project includes the following approved extras:
\begin{itemize}
  \item \textbf{User Manual:} A user manual that explains how to install, setup,
  and use the software on a day-to-day basis.
  \item \textbf{Usability Report:} A usability report will be developed that 
  will assess the systems core objectives and requirements. It will summarize 
  the results of testing as specfied in the SRS doc.
\end{itemize}

\subsection{Relevant Documentation}

The following documentation is directly relevant to the Verification and 
Validation activities for Project Proxi:
\begin{itemize}
  \item \textbf{Development Plan [\citet{DevPlan}]:} This document defines the overall project 
  workflow, timelines and milestones for the different phases of the project.
  \item \textbf{Problem Statement and Goals [\citet{ProbStatement}]:}: This document defines the problems,
   goals and stakeholders of the project.
  \item \textbf{Software Requirements Specification [\citet{SRS}]:}  Describes the 
  functional and non functional requirements of the project.
  \item \textbf{Hazard Analysis [\citet{HazardAnalysis}]:} This document identifies any potenital sources
  of harm or risk that may be associated with the project. Potential risks like
  unauthorized access, misuse or data breaches and their mitigation strategies 
  are documented here.
  \item \textbf{Usability Report[\citet{UsabilityReport}]:} Includes results of user evaluation and beta 
  testing during validation stage. It summarizes key findings related to the 
  system meeting it's design and usability goals.
  \item \textbf{User Manual [\citet{UserManual}]:} Provides documentation that explains how to 
  install, setup and use the software. Includes helpful instruction along with 
  walkthroughs and examples to help users with varying techincal proficiency.
\end{itemize}

\section{Plan}

\wss{Introduce this section.  You can provide a roadmap of the sections to
  come.}

\subsection{Verification and Validation Team}

\wss{Your teammates.  Maybe your supervisor.
  You should do more than list names.  You should say what each person's role is
  for the project's verification.  A table is a good way to summarize this information.}

\subsection{SRS Verification}

\wss{List any approaches you intend to use for SRS verification.  This may
  include ad hoc feedback from reviewers, like your classmates (like your
  primary reviewer), or you may plan for something more rigorous/systematic.}

\wss{If you have a supervisor for the project, you shouldn't just say they will
read over the SRS.  You should explain your structured approach to the review.
Will you have a meeting?  What will you present?  What questions will you ask?
Will you give them instructions for a task-based inspection?  Will you use your
issue tracker?}

\wss{Maybe create an SRS checklist?}

\subsection{Design Verification}

\wss{Plans for design verification}

\wss{The review will include reviews by your classmates}

\wss{Create a checklists?}

\subsection{Verification and Validation Plan Verification}

\wss{The verification and validation plan is an artifact that should also be
verified.  Techniques for this include review and mutation testing.}

\wss{The review will include reviews by your classmates}

\wss{Create a checklists?}

\subsection{Implementation Verification}

\wss{You should at least point to the tests listed in this document and the unit
  testing plan.}

\wss{In this section you would also give any details of any plans for static
  verification of the implementation.  Potential techniques include code
  walkthroughs, code inspection, static analyzers, etc.}

\wss{The final class presentation in CAS 741 could be used as a code
walkthrough.  There is also a possibility of using the final presentation (in
CAS741) for a partial usability survey.}

\subsection{Automated Testing and Verification Tools}

\wss{What tools are you using for automated testing.  Likely a unit testing
  framework and maybe a profiling tool, like ValGrind.  Other possible tools
  include a static analyzer, make, continuous integration tools, test coverage
  tools, etc.  Explain your plans for summarizing code coverage metrics.
  Linters are another important class of tools.  For the programming language
  you select, you should look at the available linters.  There may also be tools
  that verify that coding standards have been respected, like flake9 for
  Python.}

\wss{If you have already done this in the development plan, you can point to
that document.}

\wss{The details of this section will likely evolve as you get closer to the
  implementation.}

\subsection{Software Validation}

\wss{If there is any external data that can be used for validation, you should
  point to it here.  If there are no plans for validation, you should state that
  here.}

\wss{You might want to use review sessions with the stakeholder to check that
the requirements document captures the right requirements.  Maybe task based
inspection?}

\wss{For those capstone teams with an external supervisor, the Rev 0 demo should 
be used as an opportunity to validate the requirements.  You should plan on 
demonstrating your project to your supervisor shortly after the scheduled Rev 0 demo.  
The feedback from your supervisor will be very useful for improving your project.}

\wss{For teams without an external supervisor, user testing can serve the same purpose 
as a Rev 0 demo for the supervisor.}

\wss{This section might reference back to the SRS verification section.}

\section{System Tests}

\wss{There should be text between all headings, even if it is just a roadmap of
the contents of the subsections.}

\subsection{Tests for Functional Requirements}

\wss{Subsets of the tests may be in related, so this section is divided into
  different areas.  If there are no identifiable subsets for the tests, this
  level of document structure can be removed.}

\wss{Include a blurb here to explain why the subsections below
  cover the requirements.  References to the SRS would be good here.}

\subsubsection{Area of Testing1}

\wss{It would be nice to have a blurb here to explain why the subsections below
  cover the requirements.  References to the SRS would be good here.  If a section
  covers tests for input constraints, you should reference the data constraints
  table in the SRS.}
		
\paragraph{Title for Test}

\begin{enumerate}

\item{test-id1\\}

Control: Manual versus Automatic
					
Initial State: 
					
Input: 
					
Output: \wss{The expected result for the given inputs.  Output is not how you
are going to return the results of the test.  The output is the expected
result.}

Test Case Derivation: \wss{Justify the expected value given in the Output field}
					
How test will be performed: 
					
\item{test-id2\\}

Control: Manual versus Automatic
					
Initial State: 
					
Input: 
					
Output: \wss{The expected result for the given inputs}

Test Case Derivation: \wss{Justify the expected value given in the Output field}

How test will be performed: 

\end{enumerate}

\subsubsection{Area of Testing2}

...

\subsection{Tests for Nonfunctional Requirements}

In here specific tests are defined to help determine if Proxi's nonfunctional
requirements have been met.  Every test in this will look at the requirements
from the SRS and determine how every result will be verified.

\subsubsection{Look and Feels Requirements}

\begin{enumerate}
					
\item{Test NF.LF.1 - UI Conformance and Readability\\}
\textbf{Type:} Static inspection and Manual
          
\textbf{Initial State:} The latest UI build
          
\textbf{Input/Condition:} Walkthrough the man screens and all of the status
indicators (listening, thinking, and ready) all with a checklist.
          
\textbf{Output/Result:} Presence and visibility of main controls such as listen,
stop, and undo; minimal clutter and advanced options hidden; large targets and
a theme toggle being present. 
          
\textbf{How test will be performed:} In this two reviewers will go through the
UI and check off the items on the checklist.

\textbf{Nonfunctional Requirements Covered:} APP.1 - APP.5

\item{Test NF.LF.2 - Test Style and Iconography Check\\}
\textbf{Type:} Static inspection and Manual
          
\textbf{Initial State:} Build with speech output enabled and the UI icon set 
implemented.
          
\textbf{Input/Condition:} It will be a trigger system speech, with review 
labels, captions, and icons.
          
\textbf{Output/Result:} Plain, short labels; synchronized on screen captions for 
spoken output; consistent iconography with textual labelling.
          
\textbf{How test will be performed:} Inspecting the strings and visual elements
and flows; record any inconsistencies found with videos and screenshots for
evidence.

\textbf{Nonfunctional Requirements Covered:} STY.1 - STY.3

\end{enumerate}

\subsubsection{Usability and Humanity Requirements}

\begin{enumerate}
					
\item{NF.US.1 - Task Based Usability and Learnability Study\\}
\textbf{Type:} Dynamic with manual usability test that will have timed tasks and
a survey.
          
\textbf{Initial State:} New users will get a 5 minute orientation to Proxi.
          
\textbf{Input/Condition:} Doing the core tasks such as opening the app, reading
files, browsing, and saving files/actions.
          
\textbf{Output/Result:} A simple command button works for simple tasks; main
actions are obvious; the participants do every task without assistance; time to
do a repeat task decreases; examples on what to say are easily found; messages
are simple, risky actions are confirmed, and error messages suggest next steps.

\textbf{How test will be performed:} Observing timed tasks, collecting
completion rates and any errors.

\textbf{Nonfunctional Requirements Covered:} EOU.R.1 - EOU.R.3, LEA.R.1 - 
LEA.R.2, UAP.R.1 - UAP.R.3

\item{NF.US.2 - Accessibility, personalization, and AODA/WCAG conformance\\}
\textbf{Type:} Dynamic test and accessibility inspection
          
\textbf{Initial State:} Build with voice only pathway enabled; captions, text
scaling, and color contrast tools implemented.
          
\textbf{Input/Condition:} There is voice only interactive execution of setup
and exit; repeated dictation sessions with captions enabled and locale set to
English.
          
\textbf{Output/Result:} All actions are possible with voice only; captions,
scaling, and contrast verified; speech model adaption over user session are 
noticeable; Canadian english formats are used.

\textbf{How test will be performed:} Run some WCAG checks, verify locale outputs,
and have users test the voice only pathway.

\textbf{Nonfunctional Requirements Covered:} ACC.R.1 - ACC.R.2, PER.R.1 - 
PER.R.2

\end{enumerate}

\subsubsection{Performance Requirements}

\begin{enumerate}

\item{NF.PF.1 - Performance Study\\}
\textbf{Type:} Dynamic, mostly automatic performance testing
          
\textbf{Initial State:} Standard hardware environment with typical system load
          
\textbf{Input/Condition:} Speak a small scripted set of normal commands and a few 
malformed ones; keep one run in a quiet room and one with moderate indoor noise. 
Let the app run for a few hours
          
\textbf{Output/Result:} Respond in less than 2 seconds; actions should be done
with 80 to 90 percent accuracy; features load fast; settings are kept; rollbacks
fast.

\textbf{How test will be performed:} Time commands, tally accuracy, log resources,
and perform updates and rollbacks.

\textbf{Nonfunctional Requirements Covered:} SAL.R.1-R.2, SAF.R.1-R.2, POA.R.1-R.2,
CAP.R.1-R.2, SOE.R.1-R.2, LON.R.1-R.2.

\end{enumerate}

\subsubsection{Operational and Enviromental Requirements}

\begin{enumerate}

\item{NF.OP.1 - Environment, ecosystem fit, interfaces, packaging, and release\\}
\textbf{Type:} Dynamic and checklist based testing
          
\textbf{Initial State:} Fresh install on Windows/macOS with integrations turned
on.

\textbf{Input/Condition:} Use the app indoors, with normal environment, 
including noise, temperature, and distance; clean install; check release info

\textbf{Output/Result:} Works within env bounds; does not disrupt other apps; 
uses secure interfaces with logs; installs easily; versioned and documented
releases.

\textbf{How test will be performed:} Spot checking tasks and env, glancing at 
traffic + logs, running clean installs, and reviewing the releasing page. 

\textbf{Nonfunctional Requirements Covered:} EPE.R.1-R.4; WER.R.1-R.2; 
IAS.R.1-R.4; PRD.R.1-R.3; REL.R.1-R.2

\end{enumerate}

\subsubsection{Maintability and Support Requirements}

\begin{enumerate}

\item{NF.MS.1 - Onboarding, support, and adaptability\\}
\textbf{Type:} Light task trial and checklist based testing
          
\textbf{Initial State:} Fresh clone; Report Problem presentable and the config 
editable.

\textbf{Input/Condition:} A new person builds and runs using the README; they add
one small tool through the registry; generate support bundle; and change default 
apps via the config and retry.

\textbf{Output/Result:} The build run succeeds; the tool added without touching
others; the support bundle has logs and info, it still works after the default 
app change.

\textbf{How test will be performed:} Time the setup, add and register the tool,
click report to inspect the ZIP, and switch default apps and retest.

\textbf{Nonfunctional Requirements Covered:} 14.1-1, 14.1-2; 14.2-1, 14.2-2; 
14.3-1, 14.3-2

\end{enumerate}

\subsubsection{Security Requirements}

\begin{enumerate}

\item{NF.SEC.1 - Access, integrity, privacy, audit stance, immunity\\}
\textbf{Type:} Permission checks and quick scans

\textbf{Initial State:} Fresh install; privacy policy; dependency scanner.

\textbf{Input/Condition:} Trigger sensitive actions; look for stored and sent data;
run dependency scans.

\textbf{Output/Result:} No account needed; sensitive actions require explicit grants;
data is protected; minimal third party data accessed with consent; no audit trails
are left; no known vulnerabilities in dependencies.

\textbf{How test will be performed:} Start flows that require permissions, look 
for traffic and logs, and run dependency scans. Run SCA scans.

\textbf{Nonfunctional Requirements Covered:} ACS-01, ACS-02; INT-01; PRIV-01; 
IMM-01, IMM-02

\end{enumerate}

\subsubsection{Cultural Requirements}

\begin{enumerate}

\item{NF.CUL.1 - Language and Tone\\}
\textbf{Type:} Quick review of content

\textbf{Initial State:} Final strings and language selector.

\textbf{Input/Condition:} Scan common screens and messages. Switch languages and 
check key screens.

\textbf{Output/Result:} Polite neutral wording; multiple languages will be usable 
for many basic flows.

\textbf{How test will be performed:} Two people will go through the common screens
and messages; one person will switch languages, complete tasks, and check key 
screens.

\textbf{Nonfunctional Requirements Covered:} CULR-01, CULR-02

\end{enumerate}

\subsubsection{Compliance Requirements}

\begin{enumerate}

\item{NF.CUL.1 - Legal and standards\\}
\textbf{Type:} Checklist

\textbf{Initial State:} Licenses, privacy policy, accessibility notes in repo
and release.

\textbf{Input/Condition:} Map behaviors to legal and standards checklist. This 
includes PIPEDA, MIT.

\textbf{Output/Result:} PIPEDA IS followed; all licenses are compatible and
included. The given standards are followed correctly.

\textbf{How test will be performed:} Check policies vs behaviors, open the
LICENSE and release notes, do quick scan and spot check, and confirm protocols
are secure.

\textbf{Nonfunctional Requirements Covered:} CULR-01, CULR-02

\end{enumerate}

\subsection{Traceability Between Test Cases and Requirements}

\wss{Provide a table that shows which test cases are supporting which
  requirements.}

\section{Unit Test Description}

\wss{This section should not be filled in until after the MIS (detailed design
  document) has been completed.}

\wss{Reference your MIS (detailed design document) and explain your overall
philosophy for test case selection.}  

\wss{To save space and time, it may be an option to provide less detail in this section.  
For the unit tests you can potentially layout your testing strategy here.  That is, you 
can explain how tests will be selected for each module.  For instance, your test building 
approach could be test cases for each access program, including one test for normal behaviour 
and as many tests as needed for edge cases.  Rather than create the details of the input 
and output here, you could point to the unit testing code.  For this to work, you code 
needs to be well-documented, with meaningful names for all of the tests.}

\subsection{Unit Testing Scope}

\wss{What modules are outside of the scope.  If there are modules that are
  developed by someone else, then you would say here if you aren't planning on
  verifying them.  There may also be modules that are part of your software, but
  have a lower priority for verification than others.  If this is the case,
  explain your rationale for the ranking of module importance.}

\subsection{Tests for Functional Requirements}

\wss{Most of the verification will be through automated unit testing.  If
  appropriate specific modules can be verified by a non-testing based
  technique.  That can also be documented in this section.}

\subsubsection{Module 1}

\wss{Include a blurb here to explain why the subsections below cover the module.
  References to the MIS would be good.  You will want tests from a black box
  perspective and from a white box perspective.  Explain to the reader how the
  tests were selected.}

\begin{enumerate}

\item{test-id1\\}

Type: \wss{Functional, Dynamic, Manual, Automatic, Static etc. Most will
  be automatic}
					
Initial State: 
					
Input: 
					
Output: \wss{The expected result for the given inputs}

Test Case Derivation: \wss{Justify the expected value given in the Output field}

How test will be performed: 
					
\item{test-id2\\}

Type: \wss{Functional, Dynamic, Manual, Automatic, Static etc. Most will
  be automatic}
					
Initial State: 
					
Input: 
					
Output: \wss{The expected result for the given inputs}

Test Case Derivation: \wss{Justify the expected value given in the Output field}

How test will be performed: 

\item{...\\}
    
\end{enumerate}

\subsubsection{Module 2}

...

\subsection{Tests for Nonfunctional Requirements}

\wss{If there is a module that needs to be independently assessed for
  performance, those test cases can go here.  In some projects, planning for
  nonfunctional tests of units will not be that relevant.}

\wss{These tests may involve collecting performance data from previously
  mentioned functional tests.}

\subsubsection{Module ?}
		
\begin{enumerate}

\item{test-id1\\}

Type: \wss{Functional, Dynamic, Manual, Automatic, Static etc. Most will
  be automatic}
					
Initial State: 
					
Input/Condition: 
					
Output/Result: 
					
How test will be performed: 
					
\item{test-id2\\}

Type: Functional, Dynamic, Manual, Static etc.
					
Initial State: 
					
Input: 
					
Output: 
					
How test will be performed: 

\end{enumerate}

\subsubsection{Module ?}

...

\subsection{Traceability Between Test Cases and Modules}

\wss{Provide evidence that all of the modules have been considered.}
				
\bibliographystyle{plainnat}

\bibliography{../../refs/References}

\newpage

\section{Appendix}

This is where you can place additional information.

\subsection{Symbolic Parameters}

The definition of the test cases will call for SYMBOLIC\_CONSTANTS.
Their values are defined in this section for easy maintenance.

\subsection{Usability Survey Questions?}

\wss{This is a section that would be appropriate for some projects.}

\newpage{}
\section*{Appendix --- Reflection}

\wss{This section is not required for CAS 741}

The information in this section will be used to evaluate the team members on the
graduate attribute of Lifelong Learning.

\input{../Reflection.tex}

\begin{enumerate}
  \item What went well while writing this deliverable? 
  \item What pain points did you experience during this deliverable, and how
    did you resolve them?
  \item What knowledge and skills will the team collectively need to acquire to
  successfully complete the verification and validation of your project?
  Examples of possible knowledge and skills include dynamic testing knowledge,
  static testing knowledge, specific tool usage, Valgrind etc.  You should look to
  identify at least one item for each team member.
  \item For each of the knowledge areas and skills identified in the previous
  question, what are at least two approaches to acquiring the knowledge or
  mastering the skill?  Of the identified approaches, which will each team
  member pursue, and why did they make this choice?
\end{enumerate}

\end{document}